{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEMg3TK0eVPj"
   },
   "source": [
    "Copyright (c) MONAI Consortium  \n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");  \n",
    "you may not use this file except in compliance with the License.  \n",
    "You may obtain a copy of the License at  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;http://www.apache.org/licenses/LICENSE-2.0  \n",
    "Unless required by applicable law or agreed to in writing, software  \n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,  \n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.  \n",
    "See the License for the specific language governing permissions and  \n",
    "limitations under the License.\n",
    "\n",
    "# 3D regression example based on DenseNet\n",
    "\n",
    "This tutorial shows an example of 3D regression task based on DenseNet and array format transforms.\n",
    "\n",
    "Here, the task is given to predict the ages of subjects from MR imagee.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Project-MONAI/tutorials/blob/main/3d_regression/densenet_training_array.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2pXpPRYeVPl"
   },
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T00:20:33.913058Z",
     "start_time": "2024-06-23T00:20:31.451820Z"
    },
    "id": "dlZS78A8eVPl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "ModuleNotFoundError: No module named 'monai'\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l82hea4heVPm"
   },
   "source": [
    "## Setup imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-23T00:20:35.515047Z",
     "start_time": "2024-06-23T00:20:33.921859Z"
    },
    "id": "0mu2D_19eVPm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.5.dev2445\n",
      "Numpy version: 1.26.2\n",
      "Pytorch version: 2.5.1+cpu\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 2af9926d853086b264680adcf954bf3232f5ec32\n",
      "MONAI __file__: c:\\Users\\<username>\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.3.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "scipy version: 1.11.4\n",
      "Pillow version: 10.1.0\n",
      "Tensorboard version: 2.18.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.66.4\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.0.0\n",
      "pandas version: 2.1.3\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "import monai\n",
    "from monai.config import print_config\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    RandRotate90,\n",
    "    Resize,\n",
    "    ScaleIntensity,\n",
    ")\n",
    "from monai.networks.nets import Regressor\n",
    "from torch.utils.data import Dataset, DataLoader  # 加入 Dataset 用於自定義數據處理\n",
    "\n",
    "# 檢查是否支援 CUDA\n",
    "pin_memory = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 配置日誌輸出\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "print_config()\n",
    "\n",
    "# 自定義 Dataset 以處理 .npz 文件\n",
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, file_paths, labels, key=\"z_mu\", transform=None):\n",
    "        \"\"\"\n",
    "        :param file_paths: .npz 文件路徑列表\n",
    "        :param labels: 對應的標籤列表\n",
    "        :param key: 要讀取的鍵名 (默認為 'z_mu')\n",
    "        :param transform: 數據增強或預處理的變換\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.key = key\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 加載 .npz 文件並提取指定鍵的數據\n",
    "        file_path = self.file_paths[idx]\n",
    "        with np.load(file_path) as data:\n",
    "            image = data[self.key]  # 提取鍵 z_mu 的數據\n",
    "            # 去掉額外維度，例如 (1, 16, 32, 32, 18) -> (16, 32, 32, 18)\n",
    "            if image.ndim == 5 and image.shape[0] == 1:\n",
    "                image = np.squeeze(image, axis=0)\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # 如果有變換，對數據進行處理\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVfv5JUVR892"
   },
   "source": [
    "## Setup data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-06-23T00:20:35.518443Z"
    },
    "id": "XnU_pzCbeVPn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hikari20220126i712th\\Downloads\\IXI-T1\\MONAI\n"
     ]
    }
   ],
   "source": [
    "# Set data directory\n",
    "directory = r'C:\\Users\\Hikari20220126i712th\\Downloads\\IXI-T1\\MONAI'\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "root_dir = directory\n",
    "\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 446 images and 446 age labels.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# 讀取 Excel 檔案\n",
    "excel_file = r'C:\\Users\\Hikari20220126i712th\\Downloads\\IXI-T1\\MONAI\\MONAI_trainval.xlsx'\n",
    "df = pd.read_excel(excel_file)\n",
    "\n",
    "# 根目錄與子資料夾\n",
    "nii_dir = r'C:\\Users\\Hikari20220126i712th\\Downloads\\IXI-T1\\MONAI\\outputencode'\n",
    "\n",
    "# 初始化影像路徑和年齡清單\n",
    "images = []\n",
    "ages = []\n",
    "\n",
    "# 從 Excel 文件中動態匹配影像檔案\n",
    "for _, row in df.iterrows():\n",
    "    file_id = row[\"IXI_ID\"]  # 從 Excel 中獲取 ID（如 \"IXI002\"）\n",
    "    age = row[\"AGE\"]  # 從 Excel 中獲取對應年齡\n",
    "\n",
    "    # 根據 ID 搜索對應的已編碼壓縮檔*.npz\n",
    "    search_pattern = os.path.join(nii_dir, f\"{file_id}*.npz\")\n",
    "    matching_files = glob(search_pattern)\n",
    "\n",
    "    # 如果找到對應的 *.npz 檔案，加入清單\n",
    "    if len(matching_files) > 0:\n",
    "        file_path = matching_files[0]\n",
    "        with np.load(file_path) as data:\n",
    "            # 提取指定鍵名 'z_mu'\n",
    "            if 'z_mu' in data.files:\n",
    "                image_data = data['z_mu']\n",
    "                # 如果影像數據的第一維是 1，則刪除該維度 (從 [1, 16, 32, 32, 18] 到 [16, 32, 32, 18])\n",
    "                if image_data.ndim == 5 and image_data.shape[0] == 1:\n",
    "                    image_data = np.squeeze(image_data, axis=0)\n",
    "                images.append(image_data)\n",
    "            else:\n",
    "                print(f\"'z_mu' key not found in {file_path}\")\n",
    "        ages.append(age)  # 添加對應的年齡\n",
    "    else:\n",
    "        print(f\"File not found for ID: {file_id}\")\n",
    "\n",
    "# 將年齡轉為 NumPy 陣列，與原始程式保持一致\n",
    "ages = np.array(ages)\n",
    "\n",
    "# 確認資料載入成功\n",
    "print(f\"Loaded {len(images)} images and {len(ages)} age labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3-MIx_lSNKF"
   },
   "source": [
    "## Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yVvt3XtHeVPn",
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 365, Validation samples: 81\n",
      "Batch 1: Inputs shape torch.Size([3, 16, 32, 32, 18]), Labels shape torch.Size([3])\n",
      "Batch 2: Inputs shape torch.Size([3, 16, 32, 32, 18]), Labels shape torch.Size([3])\n",
      "Batch 3: Inputs shape torch.Size([3, 16, 32, 32, 18]), Labels shape torch.Size([3])\n",
      "Time for loading 3 batches: 0.01 seconds\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai.transforms import Compose, ScaleIntensity, Resize, RandRotate90\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 確認是否有 GPU 支援\n",
    "pin_memory = torch.cuda.is_available()\n",
    "\n",
    "# 確保數據具有正確的維度 (16, 32, 32, 18)\n",
    "for i in range(len(images)):\n",
    "    if images[i].ndim == 4 and images[i].shape[0] == 1:  # 如果形狀是 [1, 16, 32, 32, 18]\n",
    "        images[i] = np.squeeze(images[i], axis=0)  # 移除第一維度，變為 [16, 32, 32, 18]\n",
    "\n",
    "# 定義資料轉換\n",
    "train_transforms = Compose([\n",
    "    ScaleIntensity(),  # 將數據標準化到 [0, 1] 範圍\n",
    "    Resize(spatial_size=(32, 32, 18)),  # 確保影像大小正確\n",
    "    RandRotate90()  # 隨機旋轉\n",
    "])\n",
    "\n",
    "val_transforms = Compose([\n",
    "    ScaleIntensity(),\n",
    "    Resize(spatial_size=(32, 32, 18))  # 確保驗證影像大小一致\n",
    "])\n",
    "\n",
    "# 使用 train_test_split 分割資料，70% 為訓練集，30% 為驗證集\n",
    "train_images, val_images, train_ages, val_ages = train_test_split(\n",
    "    images, ages, test_size=0.18, random_state=42\n",
    ")\n",
    "\n",
    "# 檢查資料分割結果\n",
    "print(f\"Training samples: {len(train_images)}, Validation samples: {len(val_images)}\")\n",
    "\n",
    "# 定義自定義 Dataset 類\n",
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, image_data, labels, transform=None):\n",
    "        \"\"\"\n",
    "        自定義 Dataset，處理內存中的 Numpy 數據\n",
    "        :param image_data: Numpy array 列表\n",
    "        :param labels: 對應的標籤列表\n",
    "        :param transform: MONAI 的數據轉換\n",
    "        \"\"\"\n",
    "        self.image_data = image_data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.image_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "# 定義訓練資料集與 DataLoader\n",
    "train_ds = NpzDataset(image_data=train_images, labels=train_ages, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=0, pin_memory=pin_memory)\n",
    "\n",
    "# 定義驗證資料集與 DataLoader\n",
    "val_ds = NpzDataset(image_data=val_images, labels=val_ages, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=0, pin_memory=pin_memory)\n",
    "\n",
    "# 測試 DataLoader 的第一個批次\n",
    "check_ds = NpzDataset(image_data=images, labels=ages, transform=train_transforms)\n",
    "check_loader = DataLoader(check_ds, batch_size=3, num_workers=0, pin_memory=pin_memory)\n",
    "\n",
    "# 測試 DataLoader 加載效率\n",
    "start_time = time.time()\n",
    "\n",
    "for idx, (im, label) in enumerate(check_loader):\n",
    "    print(f\"Batch {idx+1}: Inputs shape {im.shape}, Labels shape {label.shape}\")\n",
    "    if idx == 2:  # 測試前三批次\n",
    "        break\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Time for loading 3 batches: {end_time - start_time:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgH8asdqSaiT"
   },
   "source": [
    "## Create model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "peVze9d7eVPo",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "model = Regressor(in_shape=[16, 32, 32, 18], out_shape=1, channels=(16, 32, 64, 128, 256), strides=(2, 2, 2, 2))#修改維度\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "# It is important that we use nn.MSELoss for regression.\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4)\n",
    "\n",
    "# start a typical PyTorch training\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "writer = SummaryWriter()\n",
    "max_epochs = 3457  # 修改 epochs 數\n",
    "\n",
    "lowest_rmse = sys.float_info.max\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "        print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "        writer.add_scalar(\"train_loss\", loss.item(), epoch_len * epoch + step)\n",
    "\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_val_outputs = []\n",
    "        for val_data in val_loader:\n",
    "            val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n",
    "            all_labels.extend(val_labels.cpu().detach().numpy())\n",
    "            with torch.no_grad():\n",
    "                val_outputs = model(val_images)\n",
    "                flattened_val_outputs = [val for sublist in val_outputs.cpu().detach().numpy() for val in sublist]\n",
    "                all_val_outputs.extend(flattened_val_outputs)\n",
    "\n",
    "        mse = np.square(np.subtract(all_labels, all_val_outputs)).mean()\n",
    "        rmse = np.sqrt(mse)\n",
    "\n",
    "        if rmse < lowest_rmse:\n",
    "            lowest_rmse = rmse\n",
    "            lowest_rmse_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), \"best_metric_model_classification3d_array.pth\")\n",
    "            print(\"saved new best metric model\")\n",
    "\n",
    "        print(f\"Current epoch: {epoch+1} current RMSE: {rmse:.4f} \")\n",
    "        print(f\"Best RMSE: {lowest_rmse:.4f} at epoch {lowest_rmse_epoch}\")\n",
    "        writer.add_scalar(\"val_rmse\", rmse, epoch + 1)\n",
    "\n",
    "print(f\"Training completed, lowest_rmse: {lowest_rmse:.4f} at epoch: {lowest_rmse_epoch}\")\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #\"last_expr\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrwW4TLneVPp"
   },
   "source": [
    "## Cleanup data directory\n",
    "\n",
    "Remove directory if a temporary was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "66kVvM1eeVPp",
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
